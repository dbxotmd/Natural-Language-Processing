{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJqskEJvozn+HbUf91GWjB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"hpSg8PI3demZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671380927772,"user_tz":-540,"elapsed":18695,"user":{"displayName":"taeseung you","userId":"18087323276648150497"}},"outputId":"16031fb0-cede-45bf-9cd6-a6c529be6c78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd drive/MyDrive/NLP"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z7Wu7m6O-fhf","executionInfo":{"status":"ok","timestamp":1671380936652,"user_tz":-540,"elapsed":242,"user":{"displayName":"taeseung you","userId":"18087323276648150497"}},"outputId":"30b4363f-2e4b-44cd-dce3-e31027b0bb49"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_hkd4MTw_Beq","executionInfo":{"status":"ok","timestamp":1671380939346,"user_tz":-540,"elapsed":209,"user":{"displayName":"taeseung you","userId":"18087323276648150497"}},"outputId":"2abc94d4-5af3-4dc6-9c32-c2b08e5a9303"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["'cw1_template 2022(1).ipynb'    \u001b[0m\u001b[01;34mSimpsons2022\u001b[0m/      Task1test.ipynb\n"," development-examples.txt.txt   Simpsons2022.zip   Task2.ipynb\n"," \u001b[01;34mproduct_reviews\u001b[0m/               Task1.ipynb\n"]}]},{"cell_type":"code","source":["pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xXPt5TFeEIxf","executionInfo":{"status":"ok","timestamp":1671380945746,"user_tz":-540,"elapsed":4016,"user":{"displayName":"taeseung you","userId":"18087323276648150497"}},"outputId":"3e738135-b5d7-4328-e0f6-0d4757122e3c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n"]}]},{"cell_type":"code","source":["import gensim\n","from gensim.models import Word2Vec\n","import os \n","import re\n","import copy\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('omw-1.4')\n","import string\n","import torchtext\n","from torchtext.data import get_tokenizer\n","from collections import Counter\n","from nltk.corpus import stopwords\n","import pandas as pd\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from nltk import cluster\n","from nltk.stem import WordNetLemmatizer\n","from random import choices\n","from nltk.corpus import stopwords, wordnet, words\n","from nltk.corpus import wordnet\n","from nltk.cluster import GAAClusterer\n","import multiprocessing\n","nltk.download('words')\n","\n","def pos_tagger (nltk_tag):\n","  if nltk_tag.startswith('J'):\n","    return wordnet.ADJ\n","  elif nltk_tag.startswith('V'):\n","    return wordnet.VERB\n","  elif nltk_tag.startswith('N'):\n","    return wordnet.NOUN\n","  elif nltk_tag.startswith('R'):\n","    return wordnet.ADV\n","  else:         \n","    return None\n","\n","def preprocess(document: str):\n","  raw = document.read()\n"," \n","  #store each lines with lower case \n","  raw_lines = [line for line in raw.lower().split(\"\\n\") if line !=\"\"]\n","\n","  # get all the setences  \n","  reviews = []\n","  review = []\n","  for i in raw_lines:\n","    #review which are divided in tag[t]\n","    if i == '[t]':\n","      reviews.append(review)\n","      review = []\n","    else:\n","      # if found ##tag then split and append in to array\n","      if \"##\" in i:\n","        i = i.split(\"##\")\n","        review.append(i[1])\n","      else:\n","        continue\n","  reviews.append(review)\n","\n","  #delete the empty array\n","  if not reviews[0]:\n","    del reviews[0]\n","\n","  #tokenize the word in review\n","  tokens = []\n","  tokenizer = get_tokenizer('basic_english')\n","  for review in reviews:\n","    for sentence in review:\n","      tokens.append(tokenizer(sentence))\n","    \n","  #lemmatization\n","  lemmatizer=WordNetLemmatizer()\n","\n","  # pos tagging\n","  token = []\n","  for sentence in tokens:\n","    pos_tagged = nltk.pos_tag(sentence)\n","    wordnet_tagged = [(x[0], pos_tagger(x[1])) for x in pos_tagged]\n","  \n","    # lemmatisation\n","    lemmatized_sentence = []\n","    for word, tag in wordnet_tagged:\n","        if tag is None:\n","            # if there is no available tag, append the token as is\n","            lemmatized_sentence.append(word)\n","        else:       \n","            # else use the tag to lemmatize the token\n","            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n","            # print(lemmatized_sentence)   \n","    token.append(lemmatized_sentence)\n","\n","  #remove some symbols still exist\n","  punctuations_symbols = \"[]≠−→←“•’´―⁄ .!?#$%^&*(){}~`|><,”'\" +  (string.punctuation)\n","  for sentence in token:\n","    for items in sentence:\n","      if items in punctuations_symbols:\n","        sentence.remove(items)\n","\n","  #remove stop words    \n","  stop_words = set(stopwords.words())\n","  \n","  tokens =[]\n","  for sentence in token:\n","    tokens.append([word for word in sentence if not word in stop_words])\n","\n","  return tokens\n","\n","\n","def main():\n","  path = \"product_reviews/product_reviews/\"\n","  filelist = os.listdir(path)\n","\n","  counters = []\n","  wordvec = []\n","  allwords = []\n","\n","  #reach each txt file and put things seperately \n","  #wordlist has the all the word to check top50 words\n","  #wordvec has the position of  all the review and words\n","  #counter store each counters for tokenized each documentary\n","  for x in filelist:\n","    if x.endswith(\".txt\"):\n","      document = open(path+x, \"r\")\n","      token = preprocess(document)\n","      wordvec+= (token)\n","      wordlist= []\n","      for i in token:\n","        wordlist+= i\n","      allwords +=  wordlist\n","      counters_oj =  Counter(wordlist)\n","      counters.append(counters_oj)\n","\n","  counter_obj = Counter(allwords)\n","  counter50 = copy.deepcopy(counter_obj)\n","\n","  dellist = []\n","\n","  # delete symmetric\n","  for i in counter_obj:\n","    if i == i[::-1]:\n","      dellist.append(i)\n","\n","  for i in dellist:\n","    del counter_obj[i]\n","    del counter50[i]\n","\n","\n","  #most 50\n","  most50 = counter50.most_common(n=50)\n","\n","  print(most50)\n","\n","  percentages = []\n","  mean =0\n","  std = 0\n","  \n","\n","  # get term for most 50 and check how many reverse word should be\n","  terms = []\n","  rest_most50 = {}\n","  for word in most50:\n","    terms.append(word[0])\n","    half = int(word[1]/2)\n","    rest = word[1] - half\n","    rest_most50[word[0]] =rest\n","\n","  #every document should have that much words\n","  data = {}\n","  for i in most50:\n","    for docu in counters:\n","      half = int(docu[i[0]]/2)\n","      rest = docu[i[0]] - half\n","      if i[0] in data:\n","        data[i[0]].append(half)\n","      else:\n","        data[i[0]] = [half]\n","\n","      if i[0][::-1] in data:\n","        data[i[0][::-1]].append(rest)\n","      else:\n","        data[i[0][::-1]] = [rest]\n","\n","\n","  #get the index of each top50 words\n","  topword_index = {}\n","  for i in range(len(wordvec)):\n","    for j in range(len(wordvec[i])):\n","      if wordvec[i][j] in terms:\n","        if wordvec[i][j] in topword_index:\n","          topword_index[wordvec[i][j]].append([i,j])\n","        else:\n","          topword_index[wordvec[i][j]] = []\n","          topword_index[wordvec[i][j]].append([i,j])\n","    \n","  # run 10 times of checking percentage of clusterting \n","  for i in range(10):\n","    #deep copy of wordvec\n","    wordvec_cp = copy.deepcopy(wordvec)\n","    terms_cp = copy.deepcopy(terms)\n","    \n","    #randomly reverse the word in the whole tokens\n","    for r_word in data:\n","      if r_word not in terms_cp:\n","        for num in range(rest_most50[r_word[:: -1]]):\n","          [pos] = choices(topword_index[r_word[::-1]])\n","          wordvec_cp[pos[0]][pos[1]] = r_word\n","  \n","    # get all terms for target and reversed one\n","    terms_all = []\n","    for i in data:\n","      terms_all.append(i)\n","\n","    cores = multiprocessing.cpu_count()\n","    #word2vec model set parameters  window size 10 min_count 5\n","    w2v  = Word2Vec(min_count=5,\n","                      window=10,\n","                      sg =1,\n","                      size=100,\n","                      workers=cores-1)\n","    #Build word2vec vocabulary\n","    w2v.build_vocab(wordvec_cp, progress_per=10000)\n","    #Traing word2vec copy every time\n","    w2v.train(wordvec_cp, total_examples=w2v.corpus_count, epochs=20, report_delay=1)\n","    # get vector of vocabulary\n","    w2v_vectors = w2v.wv.vectors\n","    # get indicies of vocabulary\n","    w2v_indices = {word: w2v.wv.vocab[word].index for word in w2v.wv.vocab}\n","\n","    # getting feature vector for each terms\n","    feature_vecs = []\n","    for i in terms_all:\n","      feature_vec = w2v_vectors[w2v_indices[i]]\n","      feature_vecs.append(feature_vec)\n","\n","    #use GAACcluster\n","    clusterer = GAAClusterer(50)\n","    labels = clusterer.cluster(feature_vecs, True)\n","    p = 0\n","    \n","    #checking labels every time\n","    for i in range(50):\n","      if labels[2*i] == labels[2*i +1]:\n","        p+=1\n","    p = p /50 *100\n","    print(\"correct pair\",p,\"percentage\")\n","    percentages.append(p)\n","  \n","  #print out mean and standar deviation of percentages\n","  percentages=  np.array(percentages)\n","  mean = np.mean(percentages)\n","  print(\"mean:\",mean)\n","  std =  np.std(percentages)\n","  print(\"standard deviation:\",std)\n","\n","result = main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NidAetxW_L0V","executionInfo":{"status":"ok","timestamp":1671386945154,"user_tz":-540,"elapsed":32527,"user":{"displayName":"taeseung you","userId":"18087323276648150497"}},"outputId":"c447a251-9313-49c0-ce79-1d8bed358575"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["[('ipod', 309), ('phone', 269), ('router', 264), ('camera', 248), ('work', 237), ('player', 237), ('great', 195), ('buy', 187), ('problem', 179), ('time', 179), ('battery', 174), ('diaper', 171), ('product', 159), ('make', 156), ('easy', 147), ('computer', 143), ('feature', 134), ('quality', 125), ('give', 119), ('micro', 119), ('find', 115), ('creative', 112), ('software', 108), ('picture', 103), ('sound', 99), ('music', 96), ('bag', 96), ('review', 94), ('purchase', 93), ('mp3', 93), ('song', 93), ('bit', 89), ('lot', 85), ('norton', 82), ('month', 81), ('year', 81), ('run', 81), ('small', 79), ('card', 78), ('file', 77), ('cd', 76), ('wireless', 76), ('service', 76), ('linksys', 76), ('apple', 75), ('program', 75), ('support', 70), ('champ', 70), ('day', 68), ('download', 67)]\n","correct pair 78.0 percentage\n","correct pair 74.0 percentage\n","correct pair 70.0 percentage\n","correct pair 72.0 percentage\n","correct pair 76.0 percentage\n","correct pair 74.0 percentage\n","correct pair 74.0 percentage\n","correct pair 80.0 percentage\n","correct pair 70.0 percentage\n","correct pair 76.0 percentage\n","mean: 74.4\n","standard deviation: 3.0724582991474434\n"]}]}]}